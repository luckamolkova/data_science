{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "**Supervised Learning**<br>\n",
    "[Linear Regression](#LinearRegression)<br> Shrinkage methods<br> - \n",
    "[Ridge Regression](#RidgeRegression)<br> - \n",
    "[Lasso Regression](#LassoRegression)<br>\n",
    "[Logistic Regression](#LogisticRegression)<br> Coefficient Optimization <br>- \n",
    "[Gradient Descent](#GradientDescent)<br> - \n",
    "[SGD](#SGD) (Stochastic Gradient Descent)<br>\n",
    "[K Nearest Neighbors](#KNN)<br>\n",
    "[Decision Tree](#DecisionTree)<br>\n",
    "[Bagging](#Bagging)<br>\n",
    "[Random Forest](#RandomForest)<br> Boosting<br> -\n",
    "[Gradient Boosting](#GradientBoost)<br> -\n",
    "[AdaBoost](#AdaBoost)<br>\n",
    "[SVM](#SVM) (Support Vector Machines)<br>\n",
    "[TF-IDF](#TfIdf)<br>\n",
    "\n",
    "**Unsupervised Learning**<br>\n",
    "[K Means](#KMeans)<br>\n",
    "[Hierarchical Clustering](#HierarchicalClustering)<br>\n",
    "[PCA](#PCA) (Principal Component Analysis)<br>\n",
    "[SVD](#SVD) (Singular Value Decomposition)<br>\n",
    "[NMF](#NMF) (Non-negative Matrix Factorization)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "data = load_iris()\n",
    "y_all = data.target\n",
    "X_all = data.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "X, X_test, y, y_test = train_test_split(X_all, y_all, \n",
    "                                        test_size=0.25, \n",
    "                                        random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='LinearRegression'></a>\n",
    "### Linear Regression\n",
    "\n",
    "[LinearRegression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) - sklearn <br>\n",
    "[OLS](http://statsmodels.sourceforge.net/devel/generated/statsmodels.regression.linear_model.OLS.html) - statsmodels\n",
    "\n",
    "**Pros**<br> -\n",
    "High accuracy<br> -\n",
    "Insensitive to outliers<br> -\n",
    "No assumptions about data<br> -\n",
    "Computationally cheap to train\n",
    "\n",
    "**Cons**<br> -\n",
    "Computationally expensive to predict<br> -\n",
    "Requires a lot of memory\n",
    "\n",
    "**Notes**<br> -\n",
    "Needs intercept (can use `fit_intercept=True` in `sklearn`)<br> -\n",
    "Needs dummies (drop one)<br> -\n",
    "Needs normalization: ?<br> -\n",
    "Needs numeric<br> -\n",
    "Target type: numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model = LinearRegression(fit_intercept=True)\n",
    "model.fit(X,y)\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "model.coef_\n",
    "model.intercept_\n",
    "accuracy = model.score(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from statsmodels.regression.linear_model import OLS\n",
    "import statsmodels.api as sm\n",
    "\n",
    "X_intercept = sm.add_constant(X)\n",
    "\n",
    "model = OLS(y,X_intercept)\n",
    "results = model.fit()\n",
    "results.summary()\n",
    "results.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id='RidgeRegression'></a>\n",
    "#### Ridge Regression\n",
    "\n",
    "[Ridge](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html)\n",
    "\n",
    "$$ Ridge\\ cost\\ function = \\sum_{i=1}^n (y_i - \\hat{y_i})^2 \n",
    "                            + \\lambda \\sum_{j=1}^p \\beta_j^2$$\n",
    "\n",
    "The second term here is the shrinkage penalty. Lambda here is an example of a tuning parameter. You will need to determine the best value of lambda via cross validation.\n",
    "\n",
    "- If we increase lambda, the variance will decrease and the bias will increase.\n",
    "- Lambda=0 is the same as standard Linear Regression.\n",
    "- Lambda in the cost function is equivalent to alpha in `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "model = Ridge(fit_intercept=True, alpha=1.)\n",
    "model.fit(X,y)\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "model.coef_\n",
    "model.intercept_\n",
    "accuracy = model.score(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='LassoRegression'></a>\n",
    "#### Lasso Regression\n",
    "[Lasso](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html)\n",
    "\n",
    "$$ Lasso\\ cost\\ function = \\sum_{i=1}^n (y_i + \\hat{y_i})^2\n",
    "                            + \\lambda \\sum_{j=1}^p \n",
    "                            \\left|\\beta_j\\right| $$\n",
    "                            \n",
    "A key difference between Lasso and Ridge is that with Lasso, some of the coefficients will go to 0, while with Ridge, they will just get small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "model = Lasso(fit_intercept=True, alpha=1.)\n",
    "model.fit(X,y)\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "model.coef_\n",
    "model.intercept_\n",
    "accuracy = model.score(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='LogisticRegression'></a>\n",
    "### Logistic Regression\n",
    "[LogisticRegression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)\n",
    "\n",
    "$$ Sigmoid\\ function = \\sigma(z) = \\frac{1}{1+e^{-z}} $$\n",
    "where $$ z = \\beta_0x_0 + \\beta_1x_1 + \\beta_2x_2 + ... + \\beta_nx_n $$\n",
    "\n",
    "**Pros**<br> -\n",
    "Computationally inexpensive<br> -\n",
    "Easy to implement<br> -\n",
    "Knowledge representation<br> -\n",
    "Easy to interpret<br>\n",
    "\n",
    "**Cons**<br> -\n",
    "Prone to underfitting<br> -\n",
    "May have low accuracy\n",
    "\n",
    "**Notes**<br> -\n",
    "Needs intercept<br> -\n",
    "Needs dummies (drop one)<br> -\n",
    "Needs normalization: ?<br> -\n",
    "Needs numeric<br> -\n",
    "Target type: numeric or nominal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(fit_intercept=True,\n",
    "                           penalty='l2', C=10.)\n",
    "#C is inverse of regularization strength (positive float)\n",
    "#smaller C means more regularization\n",
    "\n",
    "model.fit(X,y)\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "model.coef_\n",
    "model.intercept_\n",
    "accuracy = model.score(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='GradientDescent'></a>\n",
    "#### Gradient Descent\n",
    "\n",
    "Gradient descent is based on the idea that if we want to find the maximum point on a function, then the best way to move is in the direction of the gradient. \n",
    "\n",
    "$$ \\nabla f(x,y) = \\begin{pmatrix}\n",
    "   \\frac{\\delta f(x,y)}{\\delta x}\\\\\n",
    "   \\frac{\\delta f(x,y)}{\\delta y} \n",
    "   \\end{pmatrix} $$\n",
    "   \n",
    "Pseudocode:\n",
    "```\n",
    "Start with the weights all set to 1 \n",
    "Repeat R number of times:\n",
    "    Calculate the gradient of the entire dataset \n",
    "    Update the weights vector by alpha*gradient \n",
    "    Return the weights vector\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='SGD'></a>\n",
    "#### SGD (Stochastic Gradient Descent)\n",
    "\n",
    "SGD is an alternative to gradient descent method which updates the weights using only one instance at a time.\n",
    "\n",
    "Better for large datasets where regular gradient descent is unnecessarily expensive in terms of computational resources.\n",
    "\n",
    "Pseudocode:\n",
    "```\n",
    "Start with the weights all set to 1 \n",
    "For each piece of data in the dataset:\n",
    "    Calculate the gradient of one piece of data \n",
    "    Update the weights vector by alpha*gradient \n",
    "    Return the weights vector\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='KNN'></a>\n",
    "### K Nearest Neighbors\n",
    "[KNeighborsClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)\n",
    "\n",
    "**Pros**<br> - \n",
    "High accuracy<br> -\n",
    "Insensitive to outliers<br> -\n",
    "No assumptions about data<br> -\n",
    "Computationally cheap to train\n",
    "\n",
    "**Cons**<br> -\n",
    "Computationally expensive to predict<br> -\n",
    "Requires a lot of memory\n",
    "\n",
    "**Notes**<br> -\n",
    "Target type: numeric or nominal\n",
    "\n",
    "**Algorithm**<br>\n",
    "1\\. Compute the distance from your un-labeled new data point to each point in your training set.<br>\n",
    "2\\. Sort by distance.<br>\n",
    "3\\. Take the k-closest and choose the most common label.\n",
    "\n",
    "When calculating distances, use the Euclidean or Cosine distance equations.\n",
    "\n",
    "**Euclidean Distance**\n",
    "$$ dist(a,b) = \\frac{a \\cdot b}{||a|| ||b||} = \\sqrt{\\sum_i (a_i - b_i)^2} $$\n",
    "\n",
    "**Cosine Distance**\n",
    "$$ dist(a,b) = 1 - \\frac{a \\cdot b}{||a|| ||b||} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "model = KNeighborsClassifier(n_neighbors=5)\n",
    "model.fit(X,y)\n",
    "y_pred = model.predict(X)\n",
    "y_proba = model.predict_proba(X)\n",
    "\n",
    "model.classes_\n",
    "accuracy = model.score(X,y)\n",
    "#indices of k nearest neighbors\n",
    "kneighbors = model.kneighbors(X[0].reshape(1,X[0].shape[0]),\n",
    "                              return_distance=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='DecisionTree'></a>\n",
    "### Decision Tree\n",
    "[DecisionTreeClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)\n",
    "\n",
    "**Pros**<br> -\n",
    "Computationally cheap to predict<br> -\n",
    "Easy for humans to understand learned results<br> -\n",
    "Missing values OK<br> -\n",
    "Can deal with irrelevant features\n",
    "\n",
    "**Cons**<br> -\n",
    "Prone to overfitting<br> -\n",
    "Computationally expensive to train\n",
    "\n",
    "**Notes**<br> -\n",
    "Target type: numeric or nominal\n",
    "\n",
    "To determine which feature and where to split on, use Entropy or Gini Impurity equation and choose the best information gain.\n",
    "\n",
    "**Entropy**\n",
    "$$ H(S) = -\\sum_{i=1}^m P(c_i) log_2(P(c_i)) $$\n",
    "\n",
    "**Gini Impurity**\n",
    "$$ H(S) = \\sum_{i=1}^m P(c_i)(1 - P(c_i)) $$\n",
    "\n",
    "**Information Gain**\n",
    "$$ Gain(S,D) = H(S) - \\sum_{V \\in D} \\frac {|V|}{|S|} H(V)$$\n",
    "where S is the original (parent) set, D is the split, V is the subset (child) of S."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "model = DecisionTreeClassifier(criterion='gini',\n",
    "                              max_depth=3,\n",
    "                              max_features=None)\n",
    "model.fit(X,y)\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "model.classes_\n",
    "model.feature_importances_\n",
    "model.tree_\n",
    "accuracy = model.score(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='img/tree.png' width='400'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prepruning** is making the decision tree algorithm stop early. Here are a few ways that we preprune:<br> -\n",
    "Leaf size: Stop when the number of data points for a leaf gets below a threshold.<br> -\n",
    "Depth: Stop when the depth of the tree (distance from root to leaf) reaches a threshold<br> -\n",
    "Mostly the same: Stop when some percent of the data points are the same (rather than all the same)<br> -\n",
    "Error threshold: Stop when the error reduction (information gain) isn't improved significantly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Postpruning** involves building the tree first and then choosing to cut off some of the leaves.\n",
    "\n",
    "Pseudocode:\n",
    "```\n",
    "if either left or right is not a leaf:\n",
    "    call Prune on that split\n",
    "if both left and right are leaf nodes:\n",
    "    calculate error associated with merging two nodes\n",
    "    calculate error associated without merging two nodes\n",
    "    if merging results in lower error:\n",
    "        merge the leaf nodes\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Bagging'></a>\n",
    "### Bagging\n",
    "[BaggingClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html) - for classification (classes)<br>\n",
    "[BaggingRegressor](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingRegressor.html) - for regression (continuous values)\n",
    "\n",
    "Ensemble Methods combine multiple machine learning algorithms to obtain better predictive performance. The idea is simple: run multiple models on the data and use their predictions to make a prediction that is better than any of the models could do alone.\n",
    "\n",
    "Bagging, also known as bootstrap aggregating, is for running multiple models in parallel (the models don't use each other's results in order to predict). Each model gets a vote on the final prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='RandomForest'></a>\n",
    "### Random Forest\n",
    "[RandomForestClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) <br>\n",
    "[RandomForestRegressor](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html)\n",
    "\n",
    "Repeatedly randomly select data from the dataset (with replacement) and build a Decision Tree with each new sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OOB (out-of-bag) Error**\n",
    "Each tree doesn't see all of the training data, about one third of the data is left out. So we can use the skipped data to cross validate each tree individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=20, criterion='gini', \n",
    "                               max_depth=3, max_features='auto', \n",
    "                               bootstrap=True, oob_score=True,\n",
    "                               random_state=None, warm_start=False)\n",
    "model.fit(X,y)\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "model.estimators_\n",
    "model.feature_importances_\n",
    "model.oob_score_\n",
    "accuracy = model.score(X,y)\n",
    "#regressor returns coefficient of determination R^2 of the prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='GradientBoost'></a>\n",
    "#### Gradient Boosting\n",
    "[GradientBoostingClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html) <br>\n",
    "[GradientBoostingRegressor](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html)\n",
    "\n",
    "In boosting, the different classifiers are trained sequentially. In gradient boosting, each model in the ensemble is trained on the errors of the previous.\n",
    "\n",
    "*can use **trees***<br>\n",
    "*`learning_rate` shrinks the contribution of each tree by `learning_rate`; lower `learning_rate` needs more `n_estimators`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "model = GradientBoostingClassifier(loss='deviance', learning_rate=0.1, \n",
    "                                   n_estimators=100, subsample=1.0,\n",
    "                                   max_depth=3, init=None, \n",
    "                                   random_state=None, max_features=None, \n",
    "                                   verbose=0, max_leaf_nodes=None, warm_start=False)\n",
    "model.fit(X,y)\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "model.estimators_\n",
    "model.feature_importances_\n",
    "accuracy = model.score(X,y) \n",
    "#regressor returns coefficient of determination R^2 of the prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='AdaBoost'></a>\n",
    "#### AdaBoost (Adaptive Boosting)\n",
    "[AdaBoostClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html) <br>\n",
    "[AdaBoostRegressor](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostRegressor.html)\n",
    "\n",
    "**Pros**<br> -\n",
    "Low generalization error<br> -\n",
    "Easy to code<br> -\n",
    "Works with most classifiers<br> -\n",
    "No parameters to adjust\n",
    "\n",
    "**Cons**<br> -\n",
    "Sensitive to outliers\n",
    "\n",
    "**Notes**<br> -\n",
    "Target type: nominal or numerical\n",
    "\n",
    "*can use **trees or any other base estimators*** <br>\n",
    "*`learning_rate` shrinks the contribution of each tree by `learning_rate`; lower `learning_rate` needs more `n_estimators`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "\n",
    "model = AdaBoostClassifier(base_estimator=None, n_estimators=50, learning_rate=1.0)\n",
    "model.fit(X,y)\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "model.estimators_\n",
    "model.feature_importances_\n",
    "accuracy = model.score(X,y) #regressor returns coefficient of determination R^2 of the prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='SVM'></a>\n",
    "### SVM (Support Vector Machines)\n",
    "[SVC](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) (C-Support Vector Classification)\n",
    "\n",
    "**Pros**<br> -\n",
    "Low generalization error<br> -\n",
    "Computationally inexpensive<br> -\n",
    "Easy to interpret results\n",
    "\n",
    "**Cons**<br> -\n",
    "Sensitive to tuning parameters and kernel choice<br> -\n",
    "Natively only handles binary classification\n",
    "\n",
    "[Cool lecture with good visualizations](https://github.com/zipfian/DSI_Lectures/blob/master/svm/lee/SVM.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='TfIdf'></a>\n",
    "### TF-IDF (Term Frequency - Inverse Document Frequency)\n",
    "[TfidfVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)\n",
    "\n",
    "- Needs text data (single feature) as input\n",
    "- Target type: none (k-neighbors or k-means or similar method used for prediciton/classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text_data = pd.DataFrame([['After he slapped two soldiers, US Lieutenant General George S. Patton was sidelined from combat command'],\n",
    "                         ['The Fringes of the Fleet is a booklet written in 1915 by Rudyard Kipling. It contains essays and poems.'],\n",
    "                         ['Antarctica, on average, is the coldest, driest, and windiest continent, and has the highest average elevation of all the continents.']])\n",
    "text_data.columns = ['wiki_content']\n",
    "text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(input='content', lowercase=True, tokenizer=None, \n",
    "                        stop_words='english', use_idf=True)\n",
    "tfidf = vectorizer.fit_transform(text_data['wiki_content'])\n",
    "\n",
    "#each token (word) is turned into a feature, get full list like this\n",
    "vectorizer.get_feature_names()\n",
    "#tfidf is sparse matrix, to print it nicely we can convert it to an array\n",
    "tfidf.toarray()\n",
    "\n",
    "#nice output (only possible for super small sets)\n",
    "pandas_tfidf = pd.DataFrame(tfidf.toarray())\n",
    "pandas_tfidf.columns = vectorizer.get_feature_names()\n",
    "pandas_tfidf.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='KMeans'></a>\n",
    "### K Means\n",
    "[KMeans](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html)\n",
    "\n",
    "**Pros**<br> -\n",
    "Easy to implement<br>\n",
    "\n",
    "**Cons**<br> -\n",
    "Can converge at local minima<br> -\n",
    "Low on very large datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "model = KMeans(n_clusters=8, init='k-means++', \n",
    "               n_init=10, max_iter=300, tol=0.0001)\n",
    "model.fit(X,y)\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "model.cluster_centers_\n",
    "model.labels_\n",
    "model.inertia_\n",
    "score = model.score(X) #opposite of the value of X on the K-means objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='HierarchicalClustering'></a>\n",
    "### Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "\n",
    "vectorizer = TfidfVectorizer(input='content', lowercase=True, tokenizer=None, \n",
    "                        stop_words='english', use_idf=True)\n",
    "tfidf = vectorizer.fit_transform(text_data['wiki_content'])\n",
    "\n",
    "tfidf_array = tfidf.toarray()\n",
    "dist = squareform(pdist(tfidf_array))\n",
    "links = linkage(dist, method= 'complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#plot as follows\n",
    "#plt.figure(figsize=(15,5))\n",
    "#dendrogram(links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id='PCA'></a>\n",
    "### PCA (Principal Component Analysis)\n",
    "[PCA](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)\n",
    "\n",
    "Seek r-dimensional basis that best captures the variance in data. Direction with the largest projected variance is the first principal component, orthogonal direction that captures the second largest projected variance is the second principal component etc. In practice eigenvectors of covariance matrix are calculated. **PCA is a special case of SVD.**\n",
    "\n",
    "- unsupervised\n",
    "- dimensionality reduction / topic modelling - set number of latent features up front\n",
    "\n",
    "*sklearn implementation just calls numpy.linalg.svd and reduces the data, keeping r singular values.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "model = PCA(n_components=2) #number of dimensions/topics required\n",
    "model.fit(tfidf.toarray())\n",
    "pca = model.transform(tfidf.toarray())\n",
    "\n",
    "sum(model.explained_variance_ratio_) #two features explain 100% of variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'tfidf shape: %s - articles, words' % str(tfidf.toarray().shape)\n",
    "print 'pca shape:   %s - articles, topics' % str(pca.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='SVD'></a>\n",
    "### SVD (Singular Value Decomposition)\n",
    "[SVD](http://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.svd.html)\n",
    "\n",
    "- unsupervised\n",
    "- dimensionality reduction / topic modelling - returns all latent features, we can reduce the number later\n",
    "- very specific use case for recommenders (see [this blog](http://sifter.org/~simon/journal/20061211.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy.linalg import svd\n",
    "\n",
    "U, s, V = np.linalg.svd(tfidf.toarray(), full_matrices=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'tfidf shape:              %s - articles, words' % str(tfidf.toarray().shape)\n",
    "print 'U (weights) shape:        %s - articles, topics' % str(U.shape)\n",
    "print 's (singular value) shape: %s - topics (rank, diagonal of eigenvalues)' % str(s.shape)\n",
    "print 'V (features) shape:       %s - topics, words' % str(V.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#tfidf is roughly U.dot(np.diag(s)).dot(V)\n",
    "\n",
    "power = s**2\n",
    "cum_power = (np.cumsum(power))\n",
    "n = sum(cum_power < max(cum_power) * .9) #singular values (topics) needed to retain 90% of the total power\n",
    "\n",
    "#limit data to keep 90% of the total power - everything is sorted, just use first n\n",
    "U_lim = U[:,:n]\n",
    "s_lim = s[:n]\n",
    "V_lim = V[:n,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='NMF'></a>\n",
    "### NMF (Non-negative Matrix Factorization)\n",
    "[NMF](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html)\n",
    "\n",
    "Find two non-negative matrices W(m,r), H(r,n) whose product approximates the non-negative matrix V(m,n). r is set by user and can be substantially smaller than m or n.\n",
    "\n",
    "- unsupervised\n",
    "- dimensionality reduction / topic modelling - set number of latent features up front\n",
    "- needs non-negative matrix\n",
    "- best in class option for many recommendation problems\n",
    "- can fit via ALS (alternating least squares) or SGD (stochastic gradient descent)\n",
    "- regularize data to avoid overfitting\n",
    "\n",
    "*the objective function is minimized with an alternating minimization of W and H*<br>\n",
    "*use X.clip(min=0, max=X.max()) to ensure matrix is non-negative*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "\n",
    "model = NMF(n_components=2, max_iter=100) #number of dimensions/topics required\n",
    "W = model.fit_transform(tfidf.toarray())\n",
    "H = model.components_\n",
    "\n",
    "#MSE mean-squared error of (V - WH)\n",
    "print (np.array(tfidf - W.dot(H))**2).mean()\n",
    "\n",
    "#it is the same as norm of (V - WH) divided by number of elements in V\n",
    "#print np.linalg.norm(tfidf - np.dot(W, H)) / tfidf.toarray().size\n",
    "#print (np.array(tfidf - W.dot(H))**2).sum()**(0.5) / tfidf.toarray().size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'tfidf shape: %s - articles, words' % str(tfidf.toarray().shape)\n",
    "print 'W shape:     %s - articles, topics' % str(W.shape)\n",
    "print 'H shape:     %s - topics, words' % str(H.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
